<!--
<html>
<head>
<title>Schedule</title>

<style type="text/css">
body { 
    font-family:"Helvetica";
    box-sizing: border-box;
    color:#373737;
    
    font-size: 16px;
    font-family: 'Myriad Pro', Calibri, Helvetica, Arial, sans-serif;
    line-height: 1.5;
    -webkit-font-smoothing: antialiased;
}

.finishedClass { 
    background-color:#ccffcc;
}

.noClass { 
    background-color:#CCCCCC;
}

.slidesEtc {
    font-size:75%;    
}


h1, h2, h3, h4, h5, h6 {
  margin: 10px 0;
  font-weight: 700;
  color:#222222;
  font-family: 'Lucida Grande', 'Calibri', Helvetica, Arial, sans-serif;
  letter-spacing: -1px;
}

h1 {
  font-size: 32px;
  font-weight: 700;
}

h2 {
  padding-bottom: 10px;
  font-size: 28px;
  background: url('../images/bg_hr.png') repeat-x bottom;
}

h3 {
  font-size: 24px;
}

h4 {
  font-size: 21px;
}

h5 {
  font-size: 18px;
}

h6 {
  font-size: 16px;
}



</style>


<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-34189836-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>


</head>

<body>

<h1 align="center">Parallel Computing </h1>
<h5 align="center">Winter 2012</h5>
<h4 align="center">Department of Mathematics and Computer Science</h4>
<h4 align="center">Sri Sathya Sai Institute of Higher Learning, Prasanthi Nilayam, Puttaparthi</h4>
 <p align="center"> <img align = "center" src="images/swami.jpg" height = "200" width = "150" alt="Student Projects"><br> </p>

         <p align="center"> <a  href="http://www.sssihl.edu.in">SSSIHL Website</a> </p>
         <p align="center">
            <a href="index.html">Home</a> |
            <a href="schedule.html" target = "_blank">Schedule</a> |
            <a href="https://piazza.com/class#fall2012/cs1202/" target = "_blank">Piazza</a> |
            <a href="https://docs.google.com/document/d/1n4uryFOQ6n2nVYaqgpj3AKvtIE36Lia6VTmw2IrrJLU/edit" target = "_blank">Student Work</a> </p>


        
<br><br>
<table border="1" align="center" summary="" width="800">

<tr style="font-size:125%">
    <td>Date</td>
    <td>Topic</td>
    <td>Homework/<br/>Downloads</td>
    <td>Reading Materials</td>
</tr>

<tr class="finishedClass">
    <td>Wednesday, 8th August</td>
    <td>Evaluation Time: <br/> Quiz and Coding Contest<p class="slidesEtc"></p></td>
    <td><p class="slidesEtc"> Contest: <a href="contest/competition.pdf">Instructions And Rules</a><br/><br/>
    Download Tar: <a href="contest/kmeans_gpu.tar">kmeans_gpu</a><br/>
    <a href="contest/gpu_contest.ppt">gpu_contest.ppt</a> <br/>
    </p></td>
    <td>All the materials till now we have covered</td>
</tr>



<tr class="finishedClass">
    <td>Friday, 13 th July</td>
    <td>Lecture 7: <br/>Reduction on GPU (cont ...)<p class="slidesEtc"></p></td>
    <td></td>
    <td></td>
</tr>


<tr class="finishedClass">
    <td>Wednesday, 11 th July</td>
    <td>Lecture 6: <br/>Reduction on GPU<p class="slidesEtc"></p></td>
    <td><p class="slidesEtc"> Assignment 2: <a href="Assignments/Assignment2/Lab2.pdf">Lab2.pdf</a> <br/>
    Download Tar: <a href="Assignments/Assignment2/lab2_cs1302.tar">Lab2.tar</a><br/>
    </p></td>
    <td><p class="slidesEtc"><a href="materials/CUPTI_Users_Guide.pdf"> CUPTI User Guide, <br/><a href = "materials/Compute_Command_Line_Profiler_User_Guide.pdf">Profiler User Guide,<br/><a href="materials/MatrixTranspose.pdf"> Matrix Transpose. <br/><a href="http://people.maths.ox.ac.uk/gilesm/cuda/prac4/reduction.pdf"> Parallel Reduction. <br/> <a href="http://cis565-spring-2012.github.com/lectures/02-06-CUDA-Performance-1-of-2.ppt"> Parallel Reduction 2<br/> </p></td>
</tr>

</table>    

//-->

<html>
<head>
<title> Parallel Computing (CS1202)  Schedule  (Winter 2013)</title>
<link rel="stylesheet" type="text/css" href="style_boxes.css">
</head>
<body>
<h2 align="center"> Parallel Computing Schedule (Winter 2013)</h2>

<li> <b><a href="Parallel Processing.pdf">Course Syllabus</a></b>

<!--<b>This schedule is subject to change, so check frequently</b> //-->
<p>

<ul>

<li> <b>Pacheco:</b>
<em>An Introduction to Parallel Programming,</em>
by Peter Pacheco, Morgan Kaufmann (2011).
</li>

<li> <b> Quinn </b>
	<em> Parallel Programming in C with MPI and OpenMP </em>
	by M Quinn
</li>

</ul>




<p>

<div class="Tab">
<b>Lecture 1, 14th Nov  2012 (Wed):</b> Introduction
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lecture1.pdf">Lecture slides</a>
<li><b>Today's reading:</b>
<ul>
<li> <em>Pacheco</em>:  Chapter 1 (ALL)</li>
</tr>
</td>
</table>

<div class="Tab">
<b>Lecture 2, 17th Nov  2012 </b> Parallel Architectures 
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lecture2.pdf">Lecture slides</a>
<li><b>Today's reading:</b>
<ul>
<li> <em>Pacheco</em>:  Chapter 2 </li>
<li> <em> Hager</em>: Chapter 4 </li>
<li> <a href="https://computing.llnl.gov/tutorials/parallel_comp/"> LLNL Introduction to Parallel Computing </a> </li>
</tr>
</td>
</table>

<div class="Tab">
<b>Lecture 3, 20th Nov  2012 </b> Interconnection Networks
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lecture3.pdf">Lecture slides</a>
<li><b>Today's reading:</b>
<ul>
<li> <em>Pacheco</em>:  Chapter 2 (2.3.3) (ALL)</li>
<li> <em> Hager </em>: Chapter 4 (4.5) </li>
<li> <a href="http://pages.cs.wisc.edu/~tvrdik/5/html/Section5.html"> Topologies </a></li>
</tr>
</td>
</table>

<div class="Tab">
<b>Lecture 4, 27th Nov  2012 </b> Parallel Algorithm Design
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lecture4.pdf">Lecture slides</a>
<li><b>Today's reading:</b>
<ul>
<li> <em>Quinn</em>:  Chapter 3 </li>
<li> <em> A Grama</em>: Chapter 3 </li>
</tr>
</td>
</table>

<div class="Tab">
<b>Lecture 5, 30th Nov  2012 </b> Foster's Methodology: Application Examples
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lecture4_2(betterversion).pdf">Lecture slides</a>

</tr>
</td>
</table>


<div class="Tab">
<b>Lecture 6, 5th Dec  2012 </b> OpenMP 
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./OpenMPLecture.pdf">OpenMP Lecture</a>
<li> <a href="./OpenMPlab.pdf">OpenMP Lab</a>

</tr>
</td>
</table>



<div class="Tab">
<b>Lecture 7, 12th Dec  2012 </b> MPI 
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./MPILecture.pdf">MPI Lecture</a>
<li> <a href="./MPIlab.pdf">MPI Lab</a>

</tr>
</td>
</table>




<div class="TabLong">
<b>Lecture 8, 23rd Jan 2013</b>
Sieve of Eratosthenes
</div>
<table class="boxed">
<tr>
<td>
<ul>
</li>
<li> <a href="./Sieve.pdf">Lecture slides</a>

</tr>
</td>
</table>
<br/>

<!--
<div class="TabLong">
<b>Lecture 3, 1/17/12 (Tue):</b>  Stencil methods, Multicore programming 
OpenMP
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lec03.pdf">Lecture slides</a>
<li><b>Today's reading:</b>
<ul>
<li> <a href="./Stencil.html">Reader on Stencil Methods</a> 
<span class="note">Posted</span>
</li>
<li> <em>Pacheco,</em> Chapter 2: pp. 47-53</li>
<li> <em>Pacheco,</em> Chapter 5: pp. 209-225 (through <b>The parallel for Directive</b>)
</ul>
<li> Additional Reading (optional)
<ul>
<li> OpenMP programming (practical)
<ul>
<li> <a
href="http://software.intel.com/en-us/articles/getting-started-with-openmp">
Getting Started with OpenMP</a>, Intel.  </li>
<li> <a href="http://www.drdobbs.com/high-performance-computing/225702895#comments">
OpenMP: A Portable Solution for Threading</a>, by Shameem Akhter and Jason
Roberts, Dr. Dobb's Journal  (July 12, 2010).
</li>
</ul>
<li> Multiprocessors and cache coherence:
<Hennessy and Patterson, <em>Computer Architecture A Quantitative Approach,</em>
Morgan Kaufmann. 4th Ed., §4.1-2 [on reserve at the S&amp;E Library].
</li>
</ul>
</ul>
</ul>
</tr>
</td>
</table>
<br/>

<div class="TabLong">
<b>Lecture 4, 1/19/12 (Thu):</b>
More OpenMP; performance measurement and characterization.
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lec04.pdf">Lecture slides</a> 
<span class="note">Posted</span>
</li>
<li><b>Today's reading:</b>
<ul>
<li> <em>Pacheco,</em> Chapter 5: pp. 225-239 (start with &sect;5.5.1 "Caveats"  and read through the end of &sect;5.7.3); pp 251-262.
</li>
<li> <em>Pacheco,</em> Chapter 2: pp. 58-65.
</li>
</ul>
<li> <a href="http://cseweb.ucsd.edu/users/baden/Doc/openmp.html">
Resources for OpenMP</a> (including tutorials)
<li>
<a href="http://dl.acm.org/citation.cfm?id=370403">
Tiling Optimizations for 3D Scientific Computations</a>,
G. Rivera and C.-W. Tseng. 
<em>Proc. SC '00,</em> Dallas, TX, November 2000.

</ul>
</tr>
</td>
</table>
<br/>


<div class="TabLong">
<b>Lecture 5, 1/24/12 (Tue):</b>  Performance programming  of stencil methods,
Vectorization (SIMD and SSE), GPU architecture
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lec05.pdf">Lecture slides</a> 
<li> <span class="note">Programming Lab #1 Due at 9pm</span>
<li> <a href="../HW/A2/index.html">Programming Lab #2</a> (due Fri. 2/10/12)
<li><b>Today's reading:</b>
<ul>
    <li><a href="http://drdobbs.com/cpp/184401611">Programming Guidelines for Vectorizing C/C++ Compilers</a>, A. Bik et al. <em>Dr. Dobb&#8217;s Journal</em>, 2/1/03. </li>
		<li><a href="http://www.tommesani.com/SSE.html">Intel SSE</a> 
		(The first page is sufficient but  you can drill down for more details)
		</li>
<li> <a href="http://www.acmqueue.com/modules.php?name=Content&pa=showpage&pid=532">Scalable Parallel Programming with CUDA</a>, J. Nickolls et al.,
    <em>ACM Queue</em>, 6(2):40-53, March/April 2008. <em>Read only through
		page 48</em> </li>
</li>
</ul>
<li> To dig deeper (optional)
<ul>
<li> Vectorization</li>
<ul>
		<li><a
		href="http://software.intel.com/sites/products/documentation/hpc/composerxe/en-us/cpp/lin/optaps/common/optaps_vec_par.htm">Vectorization
		and Loops</a>, Intel C++ Compiler XE 12.0 User and Reference Guides,
		Document No. 323273-120U.
		<li><a href="http://developer.apple.com/hardwaredrivers/ve/sse.html">SSE Performance Programming</a> (developer.apple.com)
		<li><a href="http://hpc.sourceforge.net/">HPC Mac OS X</a>(How to obtain software
		and documentation)
    <li><a href="http://www.springerlink.com/content/9bcj0b6ycu5jcej4/">Automatic Intra-Register Vectorization for the Intel® Architecture</a>, A. Bik et al., <em>Int. J. of Parallel Programming</em>, vol 30, issue 2, pp 65-98 (2002).  &nbsp; &nbsp; &nbsp; &nbsp; <a href="http://dx.doi.org/10.1023/A:1014230429447">DOI</a></li>
    <li><a href="http://en.wikipedia.org/wiki/Automatic_vectorization">Vectorization</a> (wikipedia) </li>
    <li> <a href="http://en.wikipedia.org/wiki/Streaming_SIMD_Extensions"> Streaming SIMD Extensions</a> (Wikipedia) </li>
</ul>

<li> GPUs
<ul>
    <li><a href="http://en.wikipedia.org/wiki/CUDA">Wikipedia article on CUDA</a>.
This  is a handy reference that desribes the different device capabilities.</li>
    <li><a href="http://www.ddj.com/hpc-high-performance-computing/207402986">CUDA Supercomputing for the Masses</a>, by Rob Farber, part II of a 20-part series.</li>
</ul>
</ul>
</ul>
</tr>
</td>
</table>
<br/>

<div class="TabLong">
<b>Lecture 6, 1/26/12 (Thu):</b>
Programming with CUDA
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lec06.pdf">Lecture slides</a>  
<span class="note">Posted</span>
</li>
<li><b>Today's reading:</b>
<ul>
<li>
<em>Programming Massively Parallel Processors: A Hands-on
Approach,</em> by David Kirk and Wen-mei Hwu, Morgan Kaufmann Publishers (2010).  Chapter 3 (all), Chapter 4 (59-68)</li>
</ul>
</tr>
</td>
</table>
<br/>



<div class="TabLong">
<b>Lecture 7, 1/31/12 (Tue):</b>  Under the hood of the device
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lec07.pdf">Lecture slides</a> 
<span class="note">Posted</span>
<li><b>Today's reading:</b>
<ul>
<li><em>Programming Massively Parallel Processors: A Hands-on
Approach,</em> by David Kirk and Wen-mei Hwu, Morgan Kaufmann Publishers (2010).  Chapter 4 (71-74)</li>
<li> "Benchmarking GPUs to tune dense linear algebra,"
	by V. Volkov and J. Demmel.
	<em>Proc. 2008 ACM/IEEE Conf. on Supercomputing, </em>
	Austin, TX, Nov. 15 - 21, 2008.
	&nbsp; &nbsp;
	<a href="http://portal.acm.org/citation.cfm?id=1413402">PDF</a>
	<br><b>[Read through &sect;3.6]</b> (4 pp.)
	</br>
	</li>
</li>
</ul>
<li> To probe further (optional).
	"<a href="http://dl.acm.org/citation.cfm?id=1373197">NVIDIA Tesla: A Unified Graphics and Computing Architecture,</a>"
	by Erik Lindholm, John Nickolls, Stuart Oberman, and John Montrym.
	<em>
	IEEE Micro</em> <b>28</b>(2):39-255, March 2008, <a href="http://dx.doi.org/10.1109/MM.2008.3">DOI</a>
	[This paper is a little outdated but it discusses warp scheduling]
	</li>
</ul>
</ul>
</tr>
</td>
</table>
<br/>

<div class="TabLong">
<b>Lecture 8, 2/2/12 (Thu):</b>
Matrix Multiplication,
Using shared memory
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lec08.pdf">Lecture slides</a>  
<span class="note">Posted</span>
</li>
<li><b>Today's reading:</b>
<ul>
<li>
<em>Programming Massively Parallel Processors: A Hands-on
Approach,</em> by David Kirk and Wen-mei Hwu, Morgan Kaufmann Publishers (2010).  Chapter 5 (all). </li>
</ul>
</tr>
</td>
</table>
<br/>

<div class="TabLong">
<b>Lecture 9, 2/3/12 (Friday, Room 1202):</b>
Performance programming
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lec09.pdf">Lecture slides</a>  
<span class="note">Posted</span>
</li>
<li><b>Today's reading:</b>
<ul>
<li>
<em>Programming Massively Parallel Processors: A Hands-on
Approach,</em> by David Kirk and Wen-mei Hwu, Morgan Kaufmann Publishers (2010).  Chapter 6 (all). </li>
<li> Read, through section 5
 "Benchmarking GPUs to tune dense linear algebra,"
	by V. Volkov and J. Demmel.
	<em>Proc. 2008 ACM/IEEE Conf. on Supercomputing, </em>
	Austin, TX, Nov. 15 - 21, 2008.
	&nbsp; &nbsp;
	<a href="http://portal.acm.org/citation.cfm?id=1413402">PDF</a>
</ul>
<li><a
href="http://developer.download.nvidia.com/compute/cuda/1_1/Website/projects/reduction/doc/reduction.pdf">Presentation on Reduction</a> (NVIDIA Developer)
</ul>
</tr>
</td>
</table>
<br/>



<div class="TabLong">
<b>Lecture 10, 2/7/12 (Tuesday):</b>
Floating Point, GPUs in context
</div>

<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lec10.pdf">Lecture slides</a>
<span class="note">Posted</span>
</li>
<li><b>Today's reading:</b>
<ul>
<li>
<em>Programming Massively Parallel Processors: A Hands-on
Approach,</em> by David Kirk and Wen-mei Hwu, Morgan Kaufmann Publishers (2010).  Chapter 7 (all). </li>
<li>
<a href="http://portal.acm.org/citation.cfm?id=1816021">
Debunking the 100X GPU vs. CPU myth: an evaluation of throughput computing on
CPU and GPU</a>, by Victor W. Lee et al., ISCA &#8217;10.
<li>
<a
href="http://blogs.nvidia.com/ntersect/2010/06/gpus-are-only-up-to-14-times-faster-than-cpus-says-intel.html">Nvidia&#8217;s
blog response</a>
</li>
</ul>

<br/>
<li>To read further about IEEE Floating Point Arithmetic </li>
<ul>
<li>
<a href="http://www.cs.berkeley.edu/~wkahan/Mindless.pdf" class="external text" rel="nofollow">How Futile are Mindless Assessments of Roundoff in Floating-Point Computation</a>, W. Kahan. Professor Kahan was the leading force behind the IEEE standard, and he later  received the <a href="http://awards.acm.org/homepage.cfm?srt=all&awd=140">A. M. Turing Award</a> (in <a href="http://awards.acm.org/citation.cfm?id=1023746&srt=all&aw=140&ao=AMTURING&yr=1989">1989</a>).
</li>
<li><a href="http://grouper.ieee.org/groups/754/">IEEE 754: Standard for Binary
Floating-Point Arithmetic</a>, a web page with more resources.
</li>
<li> <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4610935"> 754-2008 IEEE Standard for Floating-Point Arithmetic</a>, 55 pages (2008))
</li>
</ul>
</li>
</ul>
</tr>
</td>
</table>
<br/>


<div class="TabLong">
<b>Lecture 11, 2/9/12 (Thursday):</b>
Parallel Programming Languages:
Cilk and UPC
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lec11.pdf">Lecture slides</a>  
<span class="note">Posted</span>
</li>
<li><b>Today's reading:</b>
<ul>
<li> M. Frigo, C. E. Leiserson, and K. H. Randall.
"The implementation of the Cilk-5 multithreaded language."
<em>SIGPLAN Notices</em> 33(5):212-223 (May. 1998).  &nbsp; &nbsp; <a href= "http://portal.acm.org/citation.cfm?doid=277652.277725">DOI</a>
(Also see <a href="http://bradley.csail.mit.edu/~bradley/hpcc06/">HPCC06 Challenge Class 2  in Cilk</a>)
<li><a href="http://software.intel.com/file/23369">
How to Survive the Multicore Software Revolution (or at Least Survive the
Hype</a>, Charles Leiserson and Ilya Mirman, CILK Arts (Intel web site,
optional)
<li>
<a href="http://upc.wikinet.org/w/images/upc/uploads/d/d4/UPC-Manual-1.2.pdf">
UPC Manual v1.2</a>, by 
S&eacute;bastien Chauvin, et al., May 2005: pp. 5-15, 19-29, 32-33, 71-74.
<br/>
<p>
There is no definitive UPC paper.
For more information about UPC, look
over some of the presentations from the
  <a href="http://upc.wikinet.org/wiki/Language_Tutorials">Language Tutorials</a> page in the
  <a href="http://upc.wikinet.org/wiki/Main_Page">UPC Wiki</a>
	or a nice set of <a href="http://www.upc.mtu.edu/tutorials.html">tutorials</a>) at the <a href="http://www.upc.mtu.edu/tutorials.html">Michigan Tech UPC web site</a>)</li>
	</p>

<LI>For Reference</li>
  <ul>
  <li> <a href="http://upc.lbl.gov/">Berkeley UPC web site</a> </li>
  <li><a href="http://upc.gwu.edu/">UPC community web site</a> </li>
</ul>

	</ul>

<li><b>For reference:</b> <a href="http://supertech.csail.mit.edu/cilk/">Cilk web page</a>
</ul>

</tr>
</td>
</table>
<br/>

<div class="TabLong">
<b>Lecture 12, 2/14/12 (Tuesday):</b>
Message passing; introduction to MPI.
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lec12.pdf">Lecture slides</a>  
<span class="note">Posted</span>
</li>
<li><b>Today's reading.</b>
<em>Pacheco</em>,  Chapter 6: pp. 83-94.
</ul>
</tr>
</td>
</table>
<br/>

<div class="TabLong">
<b>Lecture 13, 2/16/12 (Thursday):</b>
A first MPI application, Cannon&#8217;s Matrix Multiplication Algorithm,
basic collectives, managing communicators.
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lec13.pdf">Lecture slides</a> 
<span class="note">Posted</span>
</li>
<li><b>Today's reading:</b>
<ul>
<li>
<em>Pacheco</em>.  Chapter 2: pp. 37-40; Chapter 3: pp. 97-109.
</li>
<li> <em>A User&#8217;s Guide to MPI</em>, by Peter Pacheco, pp. 29-36. &nbsp;
 &nbsp;
<a href="http://www-cse.ucsd.edu/~baden/Doc/docs/MPI_Guide.pdf"><b>pdf</b></a>
<br>
(Or from from Peter Pacheco&#8217;s <em>Parallel Programming with MPI.</em> pp. 111-121).
	 </li>
</li>
<li> <a
href="http://www.cs.berkeley.edu/~demmel/cs267/lecture11/lecture11.html">Lectures Notes on Parallel Matrix Multiplication</a>, by Jim Demmel, UC Berkeley.
Read the <b>Introduction</b> and <b>Cannon&#8217;s algorithm on a 2D mesh</b>.
</ul>

<li> <b>Parallel print function.</b>
PPF is the Parallel Tools consortium's parallel print facility.
For more information, consult the
<a href="https://computation.llnl.gov/casc/ppf/">PPF web page</a>.
The software is installed on Triton in <tt> $(PUB)/lib/PPF</tt>,
examples in <tt>$(PUB)/examples/PPF</tt>
(See the 
<tt>README</tt> file for important  information about using the software).
</li>
<li> More about the trapezoidal rule</li>
<ul>
<li>
<a href="http://en.wikipedia.org/wiki/Trapezoidal_rule">Discussion</a>
</li>
<li>
<a href="http://www.csse.monash.edu.au/~lloyd/tildeAlgDS/Numerical/Integration">
Applet</a>
</li>
</ul>

</tr>
</td>
</table>

<div class="TabLong">
<b>Lecture 14, <span class="note">2/17/12 (Friday):</span></b>
Advanced collectives, SUMMA Matrix Multiplication Algorithm.
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lec14.pdf">Lecture slides</a>   
<span class="note">Posted</span>
</li>
<li><b>Today's reading:</b>
<ul>
<li> <em>Pacheco</em>, pp. 110-116.
</li>
<li> R. Van de Geign and J. Watts, <a href="http://onlinelibrary.wiley.com/doi/10.1002/(SICI)1096-9128(199704)9:4<255::AID-CPE250>3.0.CO;2-2/abstract">
SUMMA: Scalable universal matrix multiplication algorithm</a>,
<em>Concurrency: Practice and Experience,</em> 9:255-74 (1997)
</li>
</ul>
<li>Background on collective communication implementations - to probe deeper
</li>
<ul>
<li> <a href="http://www.mcs.anl.gov/~thakur/papers/ijhpca-coll.pdf"> Optimization of Collective Communication Operations in MPICH</a>, by R. Thakur, R. Rabenseifner, and W. Gropp.  <em>Int&#8217;l. J.  of High Performance Computing Applications,</em> (19)1:49-66, Spring 2005. Under the hood of MPICH
collectives.
</li>
<li>
Make Barnett et al.,
<a href="http://portal.acm.org/citation.cfm?id=602794&dl=ACM&coll=DL&CFID=17654400&CFTOKEN=43028295">
Building a high-performance collective communication library</a>,
<em>Proc. Supercomputing '94,</em> pp 107-116.
<em>Discusses van de Geijn's linear
time broadcast algorithm for long messages.</em>
</li>
<li> 
Detailed discussions about MPI collective communication (For Reference). <a
href="http://www.netlib.org/utk/papers/mpi-book/node91.html">MPI: The Complete
Reference</a>, by Marc Snir et al.
</li>
</ul>
</ul>

</tr>
</td>
</table>

<div class="TabLong">
<b>Lecture 15:</b> 2/23/12 (Thursday):</b>
Communication avoiding matrix multiplication; stencil methods;
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lec15.pdf">Lecture slides</a> 
<span class="note">Posted</span>
</li>
<li><b>Today's reading:</b>
<ul>
<li>
Edgar Solomonik and James Demmel,
<a href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2011/EECS-2011-10.pdf">
Communication-optimal parallel 2.5D matrix multiplication and LU factorization
algorithms</a>,
Tech Report <i>UCB/EECS-2011-10</i>, February 2011.
<b>Skip LU factorization (&sect;4-6, and parts of &sect;7-8)</b>
</li>
<li>Modeling the performance of an iterative method <a href="http://cseweb.ucsd.edu/classes/fa09/cse260/Lectures/perfModel.pdf"><b>pdf</b></a> </li>

  </ul>
</li>
<li>To probe further into Communication-avoiding algorithms,
visit the
<a href="http://bebop.cs.berkeley.edu/#pubs">Bebop web site</a>
(UC Berkeley) or Jim Demmel&#8217;s 
<a href="http://www.cs.berkeley.edu/~odedsc/CS294">Course on Communication-Avoiding algorithms</a>
 
</ul>
</ul>
</tr>
</td>
</table>
<br/>

<div class="TabLong">
<b>Lecture 16:</b> 2/28/12 (Tuesday):</b>
NUMA architectures and programming
<a name="current">&nbsp;</a>
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lec16.pdf">Lecture slides</a> 
<span class="note">Posted</span>
</li>
<li><b>Today's reading:</b>
<ul>
<li> "The SGI Origin: a ccNUMA highly scalable server," J. Laudon and D. Lenoski, <em>Proc. 24th ISCA</em>, pp 241-251, 1997.  <a href="http://doi.acm.org/10.1145/264107.264206">DOI</a></li>
<li><a href="../../../wi08/cse260/Lectures/Lec09/SharedMem.html">Notes on shared memory</a></li>
<li> For background material, see
Hennessy and Patterson, <em>Computer Architecture A Quantitative Approach,</em> 4th Ed.,
Morgan Kaufmann: Chapter 4, esp. &sect;4.1, &sect;4.4, &sect; 4.6,
on reserve in the S&E library.</li>
  </ul>
</li>
<li>Supplemental reading
<ul>

<li> <a
href="http://techpubs.sgi.com/library/tpl/cgi-bin/browse.cgi?coll=0650&db=bks&cmd=toc&pth=/SGI_Developer/OrOn2_PfTune">
Origin 2000 and Onyx2 Performance Tuning and Optimization Guide</a>,
Document No. 007-3430-003, SGI, 2001.</li>
<ul>
<li>
Chapter 1. Understanding SN0 Architecture</li>
<li>
Chapter 2. SN0 Memory Management</li>
<li>Chapter 8. Tuning for Parallel Processing: read sections
"Tuning Parallel Code for SN0," "Scalability and Data Placement,"
"Using Data Distribution Directives," but only read through "Understanding the
AFFINITY clases for threads (Example 8-11). There is a conventient
table of contents at the beginning of the section.</li>
</ul>
<li><a href="http://www.cs.berkeley.edu/~culler/cs258-s99/lec22.ppt">
Presentation Materials on the Origin 2000</a> (David Culler, UC Berkeley)
</li>
 
</ul>
</ul>
</tr>
</td>
</table>

<div class="TabLong">
<b>Lecture 17:</b> 3/1/12 (Thursday):</b>
Reflections on Performance
<a name="current">&nbsp;</a>
</div>
<table class="boxed">
<tr>
<td>
<ul>
    <li> No formal lecture slides; class discussion
    </li>
    <li><b>Today's reading:</b>
        <ul>
        <li><a href="http://crd-legacy.lbl.gov/~dhbailey/dhbpapers/twelve-ways.pdf">Twelve Ways to Fool the Masses When Giving. Performance Results on Parallel Computers.</a>, David H. Bailey, June 11, 1991.
				<li>
				<a
				href="http://www.hpcwire.com/hpcwire/2011-12-13/ten_ways_to_fool_the_masses_when_giving_performance_results_on_gpus.html">Ten Ways to Fool the Masses When Giving Performance Results on GPUs</a>, Scott Pakin, <em>HPCWire</em>.

        <li><a href="http://crd.lbl.gov/~dhbailey/dhbpapers/mislead.pdf">Misleading Performance Reporting in the Supercomputing Field</a>, by David H.  Bailey. <em>Scientific Programming,</em> vol. 1., No. 2 (Winter 1992), pp. 141-151.</li>

        </ul>
    </li>
 
</ul>
</tr>
</td>
</table>

<br/>
<div class="TabLong">
<b>Lectures 18 &amp; 19, </b>  Progress report presentations
</div>
<br/>
<div class="Tab">
<b>Lecture 20, 3/13/12 (Tuesday): Exascale computing </b>
</div>
<table class="boxed">
<tr>
<td>
<ul>
<li> <a href="./Lec20.pdf">Lecture slides</a> 
<span class="note">Posted</span>
<li><b>Background materials:</b>
<ul>
<li>
<a href="http://users.ece.gatech.edu/~mrichard/ExascaleComputingStudyReports/exascale_final_report_100208.pdf">
ExaScale Computing Study: Technology Challenges in Achieving Exascale
Systems</a>, Peter Kogge et al., Sept 28, 2008.
</li>
<li>
<a href="http://www.sdsc.edu/~allans/scidacsoftware.pdf">
Software Challenges in Extreme Scale Systems, V. Sarkar et al, 
</a>
J. of Physics Conf. Series Vol. 180, 2009.
</li>
<li>
<a
href="http://exascaleresearch.labworks.org/ascrOct2011/ascr/index/materials">
Advanced Archtiectures and Critical Technoglogies for Exascale Computing</a>.
US Dept of Energy, 2011. <i>Many presentations.</i> Start with
<a
href="http://exascaleresearch.labworks.org/ascrOct2011/uploads/dataforms/PRES_DOE_DOE%20Exascale%20Computing_111011.pdf">William Harrod&#8217;s Overview</a>
</li>
<li><a href="http://www.lanl.gov/orgs/hpc/salishan/salishan2011/3moore.pdf">
Data Processing in Exascale-class computer systems</a>,
Chuck Moore, April 2011.
</li>
<li><a href="http://www.exascale.org/iesp/Main_Page">Exascale.org</a>
</li>
<li><a href="http://www.zettaflops.org/">Zetta Flops</a>.
The next frontier (10<sup>21</sup> flops/sec).
<li>
<a href="http://www.scc.acad.bg/documentation/team.pdf">Overview of the
IBM Blue Gene/P project</a>, <em>IBM J. of Research and Development</em>, Vol.
52, Isssue 1/2, January 2008.  </li>
<li> Also see <a href="http://en.wikipedia.org/wiki/Blue_Gene">
Wikipedia article on Blue Gene</a>. Gives a longer term history of the project.
</li>
<li><a
href="http://workshops.alcf.anl.gov/gs10/files/2010/01/Morozov-BlueGeneP-Architecture.pdf">Blue GeneP Architecture</a> (presentation)
</ul>
</ul>

</tr>
</td>
</table>

<br/>



//-->
</body>
</html>

<!--
<p> Note: Much of the materials taken from various online course websites </p>

<div id="clustrmaps-widget"></div><script type="text/javascript">var _clustrmaps = {'url' : 'parallelcomp.github.com', 'user' : 1040468, 'server' : '4', 'id' : 'clustrmaps-widget', 'version' : 1, 'date' : '2012-09-01', 'lang' : 'en', 'corners' : 'square' };(function (){ var s = document.createElement('script'); s.type = 'text/javascript'; s.async = true; s.src = 'http://www4.clustrmaps.com/counter/map.js'; var x = document.getElementsByTagName('script')[0]; x.parentNode.insertBefore(s, x);})();</script><noscript><a href="http://www4.clustrmaps.com/user/024fe054"><img src="http://www4.clustrmaps.com/stats/maps-no_clusters/dmacssite.github.com-thumb.jpg" alt="Locations of visitors to this page" /></a></noscript>

</body>
</html>
//-->
